---
title: "project3"
author: "Andrew He"
date: "4/26/2016"
output: pdf_document
---

```{r, echo=FALSE}
library(ISLR)
library(ggplot2)
library(dplyr)
library(knitr)
library(broom)
data(Weekly)
```

## Question 1

### a.
_Produce some numerical and graphical summaries of the Weekly data. Do there appear to be patterns?_

First, let's plot: weekly return for each week, with 5 week moving-average and volume plots superimposed.

```{r}
Weekly <- data.frame(as.numeric(rownames(Weekly)), Weekly)
colnames(Weekly)[1] = "Week"

Weekly %>%
  mutate(ma=(Lag1+Lag2+Lag3+Lag4+Lag5)/5) %>%
  ggplot(aes(x=Week, y=Today, group=1)) + geom_point(color="deepskyblue") +
  geom_line(aes(y=ma), color="purple") + geom_line(aes(y=Volume), color="red")
```

The first obvious pattern is that volume increases over time. Expected behavior for a market.

One pattern to be noted are the cycles of volatility and stability. For example, weeks 150-300 have a tight spread of closes, but weeks 300-700 experiences markedly more volatility. The cycle repeats, with 700-900 being stable but 900 on being the most volatile.

Looks like there's some crazy stuff that happens from week 900 on. This is in 2007-2008ish so I presume it to be related to the economic crisis. Let's zoom in:

```{r}
Weekly %>%
  filter(Year > 2007) %>%
  mutate(ma=(Lag1+Lag2+Lag3+Lag4+Lag5)/5) %>%
  ggplot(aes(x=Week, y=Today, group=1)) + geom_point(color="deepskyblue") +
  geom_line(aes(y=ma), color="purple") + geom_line(aes(y=Volume), color="red")
```

Yup, the market crashes really hard around the 975th week, even dropping 18 points one week.

Aside from this, there is not much particularly interesting about this plot: markets go up and down.

### b.
_Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?_

```{r}
fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data=Weekly,
           family="binomial")

summary(fit)
```
  
Lag2 appears to be statistically significant with a very small p-value of 0.0296.

### c.
_Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression._

```{r}
Weekly.probs <- predict(fit, Weekly, type="response")
pred.logit <- rep("Down", length(Weekly.probs))
pred.logit[Weekly.probs>=0.5] <- "Up"

print(table(predicted=pred.logit, observed=Weekly$Direction))
```

The model correctly predicts 54/(54+430) = 11.2% of Down weeks and 557/(557+48) = 92%, for a total overall prediction accuracy of 56.1%. This logistic regression model disproprortionaly predicts Up weeks for both actual Up and Down weeks, with (430+557)/1089 = 90.6% of weeks predicted to be Up weeks.

### d.
_Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010). Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression._

```{r}
fit <- glm(Direction ~ Lag2, data=Weekly, family="binomial")

Weekly.probs <- predict(fit, Weekly, type="response")
pred.logit <- rep("Down", length(Weekly.probs))
pred.logit[Weekly.probs>=0.5] <- "Up"

print(table(predicted=pred.logit, observed=Weekly$Direction))
```

This model now even more disproportionately predicts Up weeks.

The model correctly predicts 33/(33+451) = 6.8% of Down weeks and 579/(579+26) = 95.7%, for a total of 56.2%. This logistic regression model disproprortionaly predicts Up weeks for both actual Up and Down weeks, with (451+569)/1089 = 93.7% of weeks predicted to be Up weeks.

## Question 2


### a.
_Use 10-fold cross validation to estimate prediction error for a random forest that predicts Direction using all predictors (except the Today variable) in the Weekly dataset._

```{r, warning=FALSE}
library(randomForest)

weekly_rf <- randomForest(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume +
                            Year, importance=TRUE, data=Weekly)

Weekly$rf_pred <- predict(weekly_rf, newdata=Weekly)
print(table(predicted=Weekly$rf_pred, observed=Weekly$Direction))


set.seed(1234)
k <- 10
n <- nrow(Weekly)
degrees <- seq(1,10)

fold_size <- ceiling(n/k)
permuted_indices <- rep(NA, k * fold_size)
permuted_indices[1:n] <- sample(n)
fold_indices <- matrix(permuted_indices, nc=k)

cv10_error_rates <- sapply(seq(1,k), function(fold_index) {
    test_indices <- na.omit(fold_indices[,fold_index])
    train_set <- Weekly[-test_indices,]
    test_set <- Weekly[test_indices,]
    
    tree_error <- sapply(degrees, function(deg) {
      weekly_rf <- randomForest(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + 
                                  Volume + Year, importance=TRUE, data=Weekly)
      mean(test_set$Direction != predict(weekly_rf, data=train_set))
    })
    tree_error
  })

matplot(degrees, cv10_error_rates, pch=19, type="b", lwd=1.4, cex=1.4, 
        xlab="Polynomial Degrees", ylab="10-fold CV Error Rate")
```

Error rate in this 10-fold cross-validation finds a prediction error rate of between 40% and 50% across all folds and degrees. When a random forest is generated for the entire Weekly dataset and predicted on itself, its predictions are 100% accurate, so this suggests that the random forest using every predictor is an overfit model.

### b.
_Do the same for an SVM using a "radial" kernel._

```{r}
library(e1071)

n <- nrow(Weekly)
train_indices <- sample(n, n/2)

costs <- c(.01, 1, 100)
gamma <- c(.01, 1, 100)
parameters <- expand.grid(costs, gamma)

svm_fits <- lapply(seq(nrow(parameters)), function(i) {
  svm(Direction~., data=Weekly, cost=parameters[i,1], kernel="radial", 
      gamma=parameters[i,2], subset=train_indices)
})

number_svs <- sapply(svm_fits, function(fit) fit$tot.nSV)
error_rate <- sapply(svm_fits, function(fit) {
  yhat <- predict(fit, newdata=Weekly[train_indices,])
  train <- mean(yhat != Weekly$Direction[train_indices])
  yhat <- predict(fit, newdata=Weekly[-train_indices,])
  test <- mean(yhat != Weekly$Direction[-train_indices])
  c(train=train, test=test)
})

radial_tab <- data.frame(cost=parameters[,1], gamma=parameters[,2], 
                         number_svs=number_svs, 
                         train_error=error_rate["train",]*100,
                         test_error=error_rate["test",]*100)
knitr::kable(radial_tab)
```

### c.
_Do the same for an SVM using a “polynomial” kernel._

```{r, warning=FALSE}
n <- nrow(Weekly)
train_indices <- sample(n, n/2)

costs <- c(.01, 1, 100)

svm_fits <- lapply(costs, function(cost) {
  svm(Direction~., data=Weekly, cost=cost, kernel="linear", 
      subset=train_indices)
})

number_svs <- sapply(svm_fits, function(fit) fit$tot.nSV)
error_rate <- sapply(svm_fits, function(fit) {
  yhat <- predict(fit, newdata=Weekly[train_indices,])
  train <- mean(yhat != Weekly$Direction[-train_indices])
  yhat <- predict(fit, newdata=Weekly[train_indices,])
  test <- mean(yhat != Weekly$Direction[-train_indices])
  c(train=train, test=test)
})

linear_tab <- data.frame(cost=costs, number_svs=number_svs, 
                    train_error=error_rate["train",]*100,
                    test_error=error_rate["test",]*100)
knitr::kable(linear_tab)
```

### d.
_Using a t-test (the lecture notes show how to use the lm function to do this), is prediction error for the SVM with “radial” kernel significantly better than the SVM with “polynomial” kernel._

```{r}
error_rates_radial <- data.frame(method="radial", error=radial_tab$test_error)
error_rates_linear <- data.frame(method="linear", error=linear_tab$test_error)
error_rates <- full_join(error_rates_radial, error_rates_linear)
boxplot(error~method, data=error_rates)

lm(error~method, data=error_rates) %>%
  tidy() %>%
  knitr::kable()
```

The radial kernel is not significantly better than the polynomial kernel.

### e.
_Using a t-test, is prediction error for the random forest significantly better than the SVM with “radial” kernel? Is it significantly better than the SVM with “polynomial” kernel?_

```{r}
cv10_error_rates <- data.frame(cv10_error_rates) %>%
  mutate(error=((X1+X2+X3+X4+X5+X6+X7+X8+X9+X10)/10)*100) %>%
  select(error)
error_rates_tree <- data.frame(method="tree", error=cv10_error_rates)
error_rates <- error_rates %>%
  full_join(error_rates_tree)

boxplot(error~method, data=error_rates)

lm(error~method, data=error_rates) %>%
  tidy() %>%
  knitr::kable()
```

The prediction error for the random forest is definintely not significantly better than for either the linear or radial SVM methods.